{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T13:02:17.020937Z",
     "iopub.status.busy": "2021-09-03T13:02:17.020182Z",
     "iopub.status.idle": "2021-09-03T13:02:20.429334Z",
     "shell.execute_reply": "2021-09-03T13:02:20.427969Z",
     "shell.execute_reply.started": "2021-09-03T13:02:17.020873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T14:19:22.718632Z",
     "iopub.status.busy": "2021-09-03T14:19:22.717996Z",
     "iopub.status.idle": "2021-09-03T14:19:31.790427Z",
     "shell.execute_reply": "2021-09-03T14:19:31.789354Z",
     "shell.execute_reply.started": "2021-09-03T14:19:22.718582Z"
    }
   },
   "outputs": [],
   "source": [
    "cal_df = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\n",
    "prices_df = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\n",
    "sales_df_wide = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T13:48:34.901220Z",
     "iopub.status.busy": "2021-09-03T13:48:34.900805Z",
     "iopub.status.idle": "2021-09-03T13:48:34.917085Z",
     "shell.execute_reply": "2021-09-03T13:48:34.915950Z",
     "shell.execute_reply.started": "2021-09-03T13:48:34.901159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   id          item_id      dept_id  \\\n",
      "7009  HOUSEHOLD_1_354_CA_3_evaluation  HOUSEHOLD_1_354  HOUSEHOLD_1   \n",
      "\n",
      "         cat_id store_id state_id  d_1  d_2  d_3  d_4  ...  d_1932  d_1933  \\\n",
      "7009  HOUSEHOLD     CA_3       CA    8    5    8    0  ...       2       3   \n",
      "\n",
      "      d_1934  d_1935  d_1936  d_1937  d_1938  d_1939  d_1940  d_1941  \n",
      "7009       2       2       0       5       5       0       9       1  \n",
      "\n",
      "[1 rows x 1947 columns]\n"
     ]
    }
   ],
   "source": [
    "#selecting a random data point for prediction\n",
    "input_data = sales_df_wide.sample()\n",
    "print(input_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T14:06:26.726595Z",
     "iopub.status.busy": "2021-09-03T14:06:26.726117Z",
     "iopub.status.idle": "2021-09-03T14:06:26.773617Z",
     "shell.execute_reply": "2021-09-03T14:06:26.772484Z",
     "shell.execute_reply.started": "2021-09-03T14:06:26.726560Z"
    }
   },
   "outputs": [],
   "source": [
    "def final_fun_1(input_data):\n",
    "    global cal_df,prices_df,sales_df_wide\n",
    "    #there are many NaN values in event_name and event_type fields\n",
    "    #Filling them with 'NoEvent' and 'None' resp\n",
    "    cal_df['event_name_1'].fillna('NoEvent',inplace=True)\n",
    "    cal_df['event_type_1'].fillna('None',inplace=True)\n",
    "    cal_df['event_name_2'].fillna('NoEvent',inplace=True)\n",
    "    cal_df['event_type_2'].fillna('None',inplace=True)\n",
    "    #dropping redundant columns and changing datatypes to reduce memory usage\n",
    "    cal_df.drop(['date','weekday'],axis=1,inplace=True)\n",
    "    cal_df['wm_yr_wk'] = cal_df.wm_yr_wk.astype('int16')\n",
    "    cal_df['d'] = cal_df.d.str[2:].astype('int16')\n",
    "    for col in ['wday','month','snap_CA','snap_TX','snap_WI']:\n",
    "        cal_df[col] = cal_df[col].astype('int8')\n",
    "    for col in ['event_name_1','event_type_1','event_name_2','event_type_2']:\n",
    "        cal_df[col] = cal_df[col].astype('category')\n",
    "        \n",
    "    # changing datatypes to reduce memory usage\n",
    "    prices_df['store_id'] = prices_df.store_id.astype('category')\n",
    "    prices_df['item_id'] = prices_df.item_id.astype('category')\n",
    "    prices_df['wm_yr_wk'] = prices_df.wm_yr_wk.astype('int16')\n",
    "    prices_df['sell_price'] = prices_df.sell_price.astype('float16')\n",
    "    \n",
    "    #converting the wide-form data frame to long-form so that columns from other 2 data frames can be merged\n",
    "    train_df = pd.melt(sales_df_wide,id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],\\\n",
    "                   var_name='d',value_name='units_sold')\n",
    "    \n",
    "    input_data = pd.melt(input_data,id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],\\\n",
    "                   var_name='d',value_name='units_sold') \n",
    "    \n",
    "    \n",
    "    # changing datatypes to reduce memory usage\n",
    "    train_df['d'] = train_df.d.str[2:].astype('int16')\n",
    "    input_data['d'] = input_data.d.str[2:].astype('int16')\n",
    "    \n",
    "    train_df['units_sold'] = train_df.units_sold.astype('int16')\n",
    "    input_data['units_sold'] = input_data.units_sold.astype('int16')\n",
    "    \n",
    "    for col in ['id','item_id','dept_id','cat_id','store_id','state_id']:\n",
    "        train_df[col] = train_df[col].astype('category') \n",
    "        input_data[col] = input_data[col].astype('category') \n",
    "    \n",
    "    \n",
    "    #merging with 'cal_df'\n",
    "    train_df = pd.merge(train_df,cal_df,on='d',how='left')\n",
    "    input_data = pd.merge(input_data,cal_df,on='d',how='left')\n",
    "    \n",
    "    #merging with 'prices_df'\n",
    "    train_df = pd.merge(train_df,prices_df,on=['item_id','store_id','wm_yr_wk'],how='left')\n",
    "    input_data = pd.merge(input_data,prices_df,on=['item_id','store_id','wm_yr_wk'],how='left')\n",
    "    \n",
    "    del cal_df,prices_df,sales_df_wide\n",
    "\n",
    "    #sell_price data is not available for many rows. \n",
    "    #For previous weeks filling this data by mean sell_prices for the item_id and store_id pair\n",
    "    train_df['sell_price'].fillna(train_df.groupby(['store_id','item_id'])['sell_price'].transform('mean'),inplace=True)\n",
    "\n",
    "    #calculating lag features and filling NA values with 0\n",
    "    for i in [7,14,21,28]:\n",
    "        train_df['lag_'+str(i)] = train_df.groupby('id')['units_sold'].transform(lambda x:x.shift(i))\n",
    "        train_df['lag_'+str(i)].fillna(0,inplace=True)\n",
    "\n",
    "    #ref:https://www.kaggle.com/kyakovlev/m5-lags-features\n",
    "    #rolling window mean features and filling NA values with 0\n",
    "    for i in [7,14]:\n",
    "        train_df['rolling_mean_'+str(i)] = train_df.groupby('id')['units_sold'].transform(lambda x:x.rolling(i).mean())\n",
    "        train_df['rolling_mean_'+str(i)].fillna(0,inplace=True)\n",
    "        \n",
    "    #rolling window meadion features and filling NA values with 0\n",
    "    for i in [7,14]:\n",
    "        train_df['rolling_median_'+str(i)] = train_df.groupby('id')['units_sold'].transform(lambda x:x.rolling(i).median())\n",
    "        train_df['rolling_median_'+str(i)].fillna(0,inplace=True)\n",
    "\n",
    "    #dropping features 'wm_yr_wk' and 'year' as they are similar to 'd'\n",
    "    train_df.drop(['wm_yr_wk','year'],axis=1,inplace=True)\n",
    "    input_data.drop(['wm_yr_wk','year'],axis=1,inplace=True)\n",
    "    \n",
    "    for i in ['lag_7','lag_14','lag_21','lag_28','rolling_mean_7','rolling_mean_14','rolling_median_7','rolling_median_14']:\n",
    "        train_df[i] = train_df[i].astype('float16')\n",
    "      \n",
    "    input_data = input_data.loc[input_data.d.isin(range(1914,1942))]\n",
    "    input_data = pd.merge(input_data,train_df,on=['id','d'],how='left',suffixes=(False,'_x'))\n",
    "    \n",
    "    rename = {'item_idFalse':'item_id', 'dept_idFalse':'dept_id', 'cat_idFalse':'cat_id', \\\n",
    "             'store_idFalse':'store_id', 'state_idFalse':'state_id','units_soldFalse':'units_sold',\\\n",
    "             'wdayFalse':'wday', 'monthFalse':'month', 'event_name_1False':'event_name_1',\\\n",
    "             'event_type_1False':'event_type_1', 'event_name_2False':'event_name_2',\\\n",
    "             'event_type_2False':'event_type_2', 'snap_CAFalse':'snap_CA', 'snap_TXFalse':'snap_TX',\\\n",
    "             'snap_WIFalse':'snap_WI', 'sell_priceFalse': 'sell_price'}\n",
    "    \n",
    "    input_data.rename(columns=rename,inplace=True)\n",
    "    \n",
    "    input_data.drop(['item_id_x','dept_id_x','cat_id_x','store_id_x','state_id_x','units_sold_x',\\\n",
    "                     'wday_x','month_x','event_name_1_x','event_type_1_x','event_name_2_x',\\\n",
    "                     'event_type_2_x','snap_CA_x','snap_TX_x','snap_WI_x','sell_price_x','units_sold'],\\\n",
    "                    axis=1,inplace=True)\n",
    "    \n",
    "    train_df.drop(['units_sold'],axis=1,inplace=True)\n",
    "    train_df = train_df.loc[(train_df.d.isin(range(1000,1914)))]\n",
    "\n",
    "    #feature encoding\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_df.item_id.values)\n",
    "    input_data['item_id'] = le.transform(input_data.item_id.values)\n",
    "    \n",
    "    le_id = LabelEncoder()\n",
    "    le_id.fit(train_df.id.values)\n",
    "    input_data['id'] = le_id.transform(input_data.id.values)\n",
    "    \n",
    "    le.fit(train_df.event_name_1.values)\n",
    "    input_data['event_name_1'] = le.transform(input_data.event_name_1.values)\n",
    "    \n",
    "    le.fit(train_df.event_type_1.values)\n",
    "    input_data['event_type_1'] = le.transform(input_data.event_type_1.values)\n",
    "    \n",
    "    le.fit(train_df.event_name_2.values)\n",
    "    input_data['event_name_2'] = le.transform(input_data.event_name_2.values)\n",
    "    \n",
    "    le.fit(train_df.event_type_2.values)\n",
    "    input_data['event_type_2'] = le.transform(input_data.event_type_2.values)\n",
    "\n",
    "    le.fit(train_df.dept_id.values)\n",
    "    input_data['dept_id'] = le.transform(input_data.dept_id.values)\n",
    "    \n",
    "    le.fit(train_df.cat_id.values)\n",
    "    input_data['cat_id'] = le.transform(input_data.cat_id.values)\n",
    "    \n",
    "    le.fit(train_df.store_id.values)\n",
    "    input_data['store_id'] = le.transform(input_data.store_id.values)\n",
    "    \n",
    "    le.fit(train_df.state_id.values)\n",
    "    input_data['state_id'] = le.transform(input_data.state_id.values)\n",
    "    \n",
    "    for i in ['item_id','event_name_1','event_type_1','event_name_2','event_type_2']:\n",
    "        input_data[i] = input_data[i].astype('int16')\n",
    "    \n",
    "    for i in ['dept_id','cat_id','store_id','state_id']:\n",
    "        input_data[i] = input_data[i].astype('int8')\n",
    "        \n",
    "    del train_df\n",
    "    \n",
    "    with open('../input/pickledata/Pickle_CAT.pkl', 'rb') as file:  \n",
    "        cat = pickle.load(file)\n",
    "    y_pred_cat = cat.predict(input_data)\n",
    "    \n",
    "    with open('../input/pickledata/Pickle_DT.pkl', 'rb') as file:  \n",
    "        dt = pickle.load(file)\n",
    "    y_pred_dt = dt.predict(input_data)    \n",
    "\n",
    "    with open('../input/pickledata/Pickle_LGBM.pkl', 'rb') as file:  \n",
    "        lgbm = pickle.load(file)\n",
    "    y_pred_lgbm = lgbm.predict(input_data)\n",
    "    \n",
    "    with open('../input/pickledata/Pickle_RF.pkl', 'rb') as file:  \n",
    "        rf = pickle.load(file)\n",
    "    y_pred_rf = rf.predict(input_data)    \n",
    "\n",
    "    with open('../input/pickledata/Pickle_XGB.pkl', 'rb') as file:  \n",
    "        xgb = pickle.load(file)\n",
    "    y_pred_xgb = xgb.predict(input_data)\n",
    "    \n",
    "    x_test_meta = np.vstack((y_pred_dt,y_pred_rf,y_pred_xgb,y_pred_lgbm,y_pred_cat)).T\n",
    "    del y_pred_dt,y_pred_rf,y_pred_xgb,y_pred_lgbm,y_pred_cat\n",
    "    with open('../input/pickledata/Pickle_LR.pkl', 'rb') as file:  \n",
    "        lr = pickle.load(file)\n",
    "    y_pred_lr = lr.predict(x_test_meta)\n",
    "    \n",
    "    d = {'id':le_id.inverse_transform(input_data['id'].values),'d':input_data['d'].values,'units':y_pred_lr}\n",
    "    y_pred = pd.DataFrame(data=d)\n",
    "    y_pred = y_pred.pivot_table(index='id',columns='d',values='units').reset_index()\n",
    "    names = {1942:'F1',1943:'F2',1944:'F3',1945:'F4',1946:'F5',1947:'F6',1948:'F7',1949:'F8',1950:'F9',1951:'F10',1952:'F11',1953:'F12',1954:'F13',1955:'F14',1956:'F15',1957:'F16',1958:'F17',1959:'F18',1960:'F19',1961:'F20',1962:'F21',1963:'F22',1964:'F23',1965:'F24',1966:'F25',1967:'F26',1968:'F27',1969:'F28'}\n",
    "    y_pred.rename(columns=names,inplace=True)\n",
    "    del d\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T14:06:29.938016Z",
     "iopub.status.busy": "2021-09-03T14:06:29.937549Z",
     "iopub.status.idle": "2021-09-03T14:15:36.524067Z",
     "shell.execute_reply": "2021-09-03T14:15:36.521474Z",
     "shell.execute_reply.started": "2021-09-03T14:06:29.937958Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d                               id      1914     1915      1916      1917  \\\n",
      "0  HOUSEHOLD_1_354_CA_3_evaluation  2.221088  1.91331  2.606695  2.529928   \n",
      "\n",
      "d      1918      1919      1920      1921      1922  ...      1932      1933  \\\n",
      "0  2.299988  2.991133  3.493998  2.393302  2.573507  ...  1.647694  2.031491   \n",
      "\n",
      "d      1934      1935      1936      1937      1938      1939      1940  \\\n",
      "0  2.269557  1.950168  1.458912  1.928056  2.459041  2.360754  4.042142   \n",
      "\n",
      "d      1941  \n",
      "0  3.961977  \n",
      "\n",
      "[1 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "predictions = final_fun_1(input_data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T14:15:36.526661Z",
     "iopub.status.busy": "2021-09-03T14:15:36.526230Z",
     "iopub.status.idle": "2021-09-03T14:15:36.534042Z",
     "shell.execute_reply": "2021-09-03T14:15:36.532766Z",
     "shell.execute_reply.started": "2021-09-03T14:15:36.526609Z"
    }
   },
   "outputs": [],
   "source": [
    "#function to calculate assymmetric rmse,custom metric function\n",
    "def armse(y_act,y_pred):\n",
    "    score=0\n",
    "    n = len(y_act)\n",
    "    diff = np.array(y_pred) - np.array(y_act)\n",
    "    for ele in diff:\n",
    "        if ele<0:\n",
    "            score += 4*(ele**2)\n",
    "        else:\n",
    "            score += ele**2\n",
    "    return np.sqrt(score/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T14:19:03.047388Z",
     "iopub.status.busy": "2021-09-03T14:19:03.046784Z",
     "iopub.status.idle": "2021-09-03T14:19:03.097390Z",
     "shell.execute_reply": "2021-09-03T14:19:03.096232Z",
     "shell.execute_reply.started": "2021-09-03T14:19:03.047352Z"
    }
   },
   "outputs": [],
   "source": [
    "def final_fun_2(input_data,y):\n",
    "    global cal_df,prices_df,sales_df_wide\n",
    "    #there are many NaN values in event_name and event_type fields\n",
    "    #Filling them with 'NoEvent' and 'None' resp\n",
    "    cal_df['event_name_1'].fillna('NoEvent',inplace=True)\n",
    "    cal_df['event_type_1'].fillna('None',inplace=True)\n",
    "    cal_df['event_name_2'].fillna('NoEvent',inplace=True)\n",
    "    cal_df['event_type_2'].fillna('None',inplace=True)\n",
    "    #dropping redundant columns and changing datatypes to reduce memory usage\n",
    "    cal_df.drop(['date','weekday'],axis=1,inplace=True)\n",
    "    cal_df['wm_yr_wk'] = cal_df.wm_yr_wk.astype('int16')\n",
    "    cal_df['d'] = cal_df.d.str[2:].astype('int16')\n",
    "    for col in ['wday','month','snap_CA','snap_TX','snap_WI']:\n",
    "        cal_df[col] = cal_df[col].astype('int8')\n",
    "    for col in ['event_name_1','event_type_1','event_name_2','event_type_2']:\n",
    "        cal_df[col] = cal_df[col].astype('category')\n",
    "        \n",
    "    # changing datatypes to reduce memory usage\n",
    "    prices_df['store_id'] = prices_df.store_id.astype('category')\n",
    "    prices_df['item_id'] = prices_df.item_id.astype('category')\n",
    "    prices_df['wm_yr_wk'] = prices_df.wm_yr_wk.astype('int16')\n",
    "    prices_df['sell_price'] = prices_df.sell_price.astype('float16')\n",
    "    \n",
    "    #converting the wide-form data frame to long-form so that columns from other 2 data frames can be merged\n",
    "    train_df = pd.melt(sales_df_wide,id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],\\\n",
    "                   var_name='d',value_name='units_sold')\n",
    "    \n",
    "    input_data = pd.melt(input_data,id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],\\\n",
    "                   var_name='d',value_name='units_sold') \n",
    "    \n",
    "    \n",
    "    # changing datatypes to reduce memory usage\n",
    "    train_df['d'] = train_df.d.str[2:].astype('int16')\n",
    "    input_data['d'] = input_data.d.str[2:].astype('int16')\n",
    "    \n",
    "    train_df['units_sold'] = train_df.units_sold.astype('int16')\n",
    "    input_data['units_sold'] = input_data.units_sold.astype('int16')\n",
    "    \n",
    "    for col in ['id','item_id','dept_id','cat_id','store_id','state_id']:\n",
    "        train_df[col] = train_df[col].astype('category') \n",
    "        input_data[col] = input_data[col].astype('category') \n",
    "    \n",
    "    \n",
    "    #merging with 'cal_df'\n",
    "    train_df = pd.merge(train_df,cal_df,on='d',how='left')\n",
    "    input_data = pd.merge(input_data,cal_df,on='d',how='left')\n",
    "    \n",
    "    #merging with 'prices_df'\n",
    "    train_df = pd.merge(train_df,prices_df,on=['item_id','store_id','wm_yr_wk'],how='left')\n",
    "    input_data = pd.merge(input_data,prices_df,on=['item_id','store_id','wm_yr_wk'],how='left')\n",
    "    \n",
    "    del cal_df,prices_df,sales_df_wide\n",
    "\n",
    "    #sell_price data is not available for many rows. \n",
    "    #For previous weeks filling this data by mean sell_prices for the item_id and store_id pair\n",
    "    train_df['sell_price'].fillna(train_df.groupby(['store_id','item_id'])['sell_price'].transform('mean'),inplace=True)\n",
    "\n",
    "    #calculating lag features and filling NA values with 0\n",
    "    for i in [7,14,21,28]:\n",
    "        train_df['lag_'+str(i)] = train_df.groupby('id')['units_sold'].transform(lambda x:x.shift(i))\n",
    "        train_df['lag_'+str(i)].fillna(0,inplace=True)\n",
    "\n",
    "    #ref:https://www.kaggle.com/kyakovlev/m5-lags-features\n",
    "    #rolling window mean features and filling NA values with 0\n",
    "    for i in [7,14]:\n",
    "        train_df['rolling_mean_'+str(i)] = train_df.groupby('id')['units_sold'].transform(lambda x:x.rolling(i).mean())\n",
    "        train_df['rolling_mean_'+str(i)].fillna(0,inplace=True)\n",
    "        \n",
    "    #rolling window meadion features and filling NA values with 0\n",
    "    for i in [7,14]:\n",
    "        train_df['rolling_median_'+str(i)] = train_df.groupby('id')['units_sold'].transform(lambda x:x.rolling(i).median())\n",
    "        train_df['rolling_median_'+str(i)].fillna(0,inplace=True)\n",
    "\n",
    "    #dropping features 'wm_yr_wk' and 'year' as they are similar to 'd'\n",
    "    train_df.drop(['wm_yr_wk','year'],axis=1,inplace=True)\n",
    "    input_data.drop(['wm_yr_wk','year'],axis=1,inplace=True)\n",
    "    \n",
    "    for i in ['lag_7','lag_14','lag_21','lag_28','rolling_mean_7','rolling_mean_14','rolling_median_7','rolling_median_14']:\n",
    "        train_df[i] = train_df[i].astype('float16')\n",
    "      \n",
    "    input_data = input_data.loc[input_data.d.isin(range(1914,1942))]\n",
    "    input_data = pd.merge(input_data,train_df,on=['id','d'],how='left',suffixes=(False,'_x'))\n",
    "    \n",
    "    rename = {'item_idFalse':'item_id', 'dept_idFalse':'dept_id', 'cat_idFalse':'cat_id', \\\n",
    "             'store_idFalse':'store_id', 'state_idFalse':'state_id','units_soldFalse':'units_sold',\\\n",
    "             'wdayFalse':'wday', 'monthFalse':'month', 'event_name_1False':'event_name_1',\\\n",
    "             'event_type_1False':'event_type_1', 'event_name_2False':'event_name_2',\\\n",
    "             'event_type_2False':'event_type_2', 'snap_CAFalse':'snap_CA', 'snap_TXFalse':'snap_TX',\\\n",
    "             'snap_WIFalse':'snap_WI', 'sell_priceFalse': 'sell_price'}\n",
    "    \n",
    "    input_data.rename(columns=rename,inplace=True)\n",
    "    \n",
    "    input_data.drop(['item_id_x','dept_id_x','cat_id_x','store_id_x','state_id_x','units_sold_x',\\\n",
    "                     'wday_x','month_x','event_name_1_x','event_type_1_x','event_name_2_x',\\\n",
    "                     'event_type_2_x','snap_CA_x','snap_TX_x','snap_WI_x','sell_price_x','units_sold'],\\\n",
    "                    axis=1,inplace=True)\n",
    "    \n",
    "    train_df.drop(['units_sold'],axis=1,inplace=True)\n",
    "    train_df = train_df.loc[(train_df.d.isin(range(1000,1914)))]\n",
    "\n",
    "    #feature encoding\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_df.item_id.values)\n",
    "    input_data['item_id'] = le.transform(input_data.item_id.values)\n",
    "    \n",
    "    le_id = LabelEncoder()\n",
    "    le_id.fit(train_df.id.values)\n",
    "    input_data['id'] = le_id.transform(input_data.id.values)\n",
    "    \n",
    "    le.fit(train_df.event_name_1.values)\n",
    "    input_data['event_name_1'] = le.transform(input_data.event_name_1.values)\n",
    "    \n",
    "    le.fit(train_df.event_type_1.values)\n",
    "    input_data['event_type_1'] = le.transform(input_data.event_type_1.values)\n",
    "    \n",
    "    le.fit(train_df.event_name_2.values)\n",
    "    input_data['event_name_2'] = le.transform(input_data.event_name_2.values)\n",
    "    \n",
    "    le.fit(train_df.event_type_2.values)\n",
    "    input_data['event_type_2'] = le.transform(input_data.event_type_2.values)\n",
    "\n",
    "    le.fit(train_df.dept_id.values)\n",
    "    input_data['dept_id'] = le.transform(input_data.dept_id.values)\n",
    "    \n",
    "    le.fit(train_df.cat_id.values)\n",
    "    input_data['cat_id'] = le.transform(input_data.cat_id.values)\n",
    "    \n",
    "    le.fit(train_df.store_id.values)\n",
    "    input_data['store_id'] = le.transform(input_data.store_id.values)\n",
    "    \n",
    "    le.fit(train_df.state_id.values)\n",
    "    input_data['state_id'] = le.transform(input_data.state_id.values)\n",
    "    \n",
    "    for i in ['item_id','event_name_1','event_type_1','event_name_2','event_type_2']:\n",
    "        input_data[i] = input_data[i].astype('int16')\n",
    "    \n",
    "    for i in ['dept_id','cat_id','store_id','state_id']:\n",
    "        input_data[i] = input_data[i].astype('int8')\n",
    "        \n",
    "    del train_df\n",
    "    \n",
    "    with open('../input/pickledata/Pickle_CAT.pkl', 'rb') as file:  \n",
    "        cat = pickle.load(file)\n",
    "    y_pred_cat = cat.predict(input_data)\n",
    "    \n",
    "    with open('../input/pickledata/Pickle_DT.pkl', 'rb') as file:  \n",
    "        dt = pickle.load(file)\n",
    "    y_pred_dt = dt.predict(input_data)    \n",
    "\n",
    "    with open('../input/pickledata/Pickle_LGBM.pkl', 'rb') as file:  \n",
    "        lgbm = pickle.load(file)\n",
    "    y_pred_lgbm = lgbm.predict(input_data)\n",
    "    \n",
    "    with open('../input/pickledata/Pickle_RF.pkl', 'rb') as file:  \n",
    "        rf = pickle.load(file)\n",
    "    y_pred_rf = rf.predict(input_data)    \n",
    "\n",
    "    with open('../input/pickledata/Pickle_XGB.pkl', 'rb') as file:  \n",
    "        xgb = pickle.load(file)\n",
    "    y_pred_xgb = xgb.predict(input_data)\n",
    "    \n",
    "    x_test_meta = np.vstack((y_pred_dt,y_pred_rf,y_pred_xgb,y_pred_lgbm,y_pred_cat)).T\n",
    "    del y_pred_dt,y_pred_rf,y_pred_xgb,y_pred_lgbm,y_pred_cat\n",
    "    with open('../input/pickledata/Pickle_LR.pkl', 'rb') as file:  \n",
    "        lr = pickle.load(file)\n",
    "        \n",
    "    y_pred_lr = lr.predict(x_test_meta)\n",
    "    \n",
    "    armse_score = armse(y,y_pred_lr)\n",
    "    rmse_score = mse(y,y_pred_lr,squared=False)\n",
    "    return armse_score,rmse_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T14:19:07.309835Z",
     "iopub.status.busy": "2021-09-03T14:19:07.309186Z",
     "iopub.status.idle": "2021-09-03T14:19:07.322920Z",
     "shell.execute_reply": "2021-09-03T14:19:07.321047Z",
     "shell.execute_reply.started": "2021-09-03T14:19:07.309783Z"
    }
   },
   "outputs": [],
   "source": [
    "lst = list(range(1914,1942))\n",
    "lst = ['d_'+str(x) for x in lst]\n",
    "y = []\n",
    "for i,row in input_data.iterrows():\n",
    "    y.extend(row[lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-03T14:19:44.089724Z",
     "iopub.status.busy": "2021-09-03T14:19:44.089308Z",
     "iopub.status.idle": "2021-09-03T14:28:38.195218Z",
     "shell.execute_reply": "2021-09-03T14:28:38.194218Z",
     "shell.execute_reply.started": "2021-09-03T14:19:44.089687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARMSE: 3.432830810585946, RMSE: 2.000308224671881\n"
     ]
    }
   ],
   "source": [
    "armse_score,rmse_score = final_fun_2(input_data,y)\n",
    "print(\"ARMSE: {}, RMSE: {}\".format(armse_score,rmse_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
